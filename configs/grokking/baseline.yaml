__include__: ['base_config.yaml']

# Reproduces author results with `weight_decay=0` which takes long time (100k steps vs 10k steps)

data:
  module_kwargs:
    prime: &prime 97

tokenizer:
  module_kwargs:
    prime: *prime

optimizer:
  module_kwargs:
    weight_decay: 0.0     # weight_decay=1 makes convergence much faster and original graph is not reproducible

training:
  max_steps: 1000000    # 1e5 is not enough when weight_decay=0.0
