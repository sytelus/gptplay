__include__: ['base_config.yaml']

general:
  out_dir: '~/out_dir/gpt2_124m_owt'

logging:
  project_name: 'gpt2_124m_owt'
  enable_wandb: true
  enable_summaries: true
  run_name: 'gpt2_124m_owt_byte'  # select random name
  run_description: 'GPT2 124M on OpenWebText with byte tokenizert'

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/openwebtext/byte/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/openwebtext/byte/val.bin'

training:
  train_batch_size: 32
  num_steps: 2000000    # for OpenWebText/9B tokens
  enable_train_log: true
  log_every: 100
  grad_clip: 1.0 # disabled if 0.0
  gradient_accumulation_steps: 8
  adj_grad_acc_gpu_count: true # adjust gradient accumulation steps for number of GPUs

model:
  module: 'nanugpt.models.nanogpt.get_model'
  module_kwargs:
    n_layer: 12
    n_embd: 768
    n_head: 12
    context_length: 2048
