__include__: ['base_config.yaml']

general:
  out_dir: '~/out_dir/gpt2_124m_owt'

logging:
  project_name: 'gpt2_124m_owt'
  enable_wandb: true
  enable_summaries: true
  run_name: 'gpt2_124m_owt_byte'  # select random name
  run_description: 'GPT2 124M on OpenWebText with byte tokenizer, 4X context and steps'

tokenizer:
  module: 'nanugpt.tokenizers.byte_tokenizer.get_tokenizer_factory'
  module_kwargs:
    encoding_name: 'utf-8'

loss:
  module: 'nanugpt.losses.autoregressive_loss.get_loss'

data:
  module: 'nanugpt.data.tokenized_data.get_data'
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/openwebtext/byte/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/openwebtext/byte/validation.bin'

training:
  train_batch_size: 60
  num_steps: 1800000    # for OpenWebText/9B tokens
  gradient_accumulation_steps: 8

model:
  module: 'nanugpt.models.nanogpt.get_model'
  module_kwargs:
    n_layer: 12
    n_embd: 768
    n_head: 12
    context_length: 4096