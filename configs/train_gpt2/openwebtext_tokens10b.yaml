__include__: ['base_config.yaml']

# config for training GPT2 on OpenWebText with 10B tokens

general:
  project_name: 'gpt2_owt'

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/openwebtext/tiktoken/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/openwebtext/tiktoken/val.bin'

training:
  device_batch_size: 12 # default 12, GH200: 72
  max_steps: 20345

eval:
  eval_every: 1000
  eval_iters: 200 # number of samples to evaluate for dataset
  save_checkpoint: true
  checkoint_after: 0 # starts saving checkpoint after these steps
  checkpoint_every_hr: 200 # multiple of eval_every, save checkpoint after these steps from last checkpoint when val loss is better
  checkpoint_keep_best: false # keep only the best checkpoint, otherwise keep all with  _{step}.pt