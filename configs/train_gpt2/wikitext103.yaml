__include__: ['base_config.yaml']

general:
  project_name: 'gpt2o_wt103'

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/tiktoken/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/tiktoken/validation.bin'
    tokenized_test_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/tiktoken/test.bin'

# model:
#   module: 'nanugpt.models.nanogpt.get_model'
#   module_kwargs:
#     n_layer: 24
#     n_embd: 1024
#     n_head: 16
#     context_length: 1024

training:
  #device_batch_size: 32
  max_steps: 4500 # for 8k, val loss stops improving after 4k
  #global_batch_size:: 512

optimizer:
  module_kwargs:
    learning_rate: 73.0E-4 # LR found by LRRT

# scheduler:
#   module: 'nanugpt.schedulers.linear.get_scheduler'
#   module_kwargs:
#     warmup_iters: 3000
#     end_factor: 1.0E-2

