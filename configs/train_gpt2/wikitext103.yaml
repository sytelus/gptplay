__include__: ['base_config.yaml']

general:
  project_name: 'gpt2o_wt103'

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/tiktoken/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/tiktoken/validation.bin'
    tokenized_test_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/tiktoken/test.bin'

# model:
#   module: 'nanugpt.models.nanogpt.get_model'
#   module_kwargs:
#     n_layer: 24
#     n_embd: 1024
#     n_head: 16
#     context_length: 1024

training:
  max_steps: 5000 # for 8k steps, val loss stops improving after 4k for ctx=1024

optimizer:
  module_kwargs:
    learning_rate: 12.0E-4 # LRRT is good unil 500E-4 but NaNs starts flowing at 200E-4

scheduler:
  module_kwargs:
    warmup_iters: 500

