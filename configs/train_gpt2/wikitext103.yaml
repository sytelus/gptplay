__include__: ['base_config.yaml']

general:
  out_dir: '~/out_dir/gpt2_124m_wikitext-103_byte'

logging:
  project_name: 'gpt2_124m_wikitext-103_byte'
  enable_wandb: true
  enable_summaries: true
  run_name: 'gpt124m_wikitext103_byte_bloat16'  # select random name
  run_description: 'GPT124M WikiText-103 using gpt2 tokenizer and bfloat16'

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/byte/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/byte/validation.bin'
    tokenized_test_path: '$DATA_ROOT/tokenized/wikitext-103-raw-v1/byte/test.bin'

model:
  module: 'nanugpt.models.nanogpt.get_model'
  module_kwargs:
    n_layer: 24
    n_embd: 1024
    n_head: 16
    context_length: 2048

training:
  train_batch_size: 16
  num_steps: 12000000
  gradient_accumulation_steps: 32
  adj_grad_acc_gpu_count: true # adjust gradient accumulation steps for number of GPUs

optimizer:
  module: 'nanugpt.optimizers.adamw_nanogpt.get_optim'
  module_kwargs:
    learning_rate: 1.5E-4

scheduler:
  module: 'nanugpt.schedulers.nanogpt_cosine.get_scheduler'
  module_kwargs:
    warmup_iters: 3000
    min_lr: 1.0E-5
