__include__: ['base_config.yaml']

general:
  project_name: 'gpt2o_tinyshakespeare'

model:
  module: 'nanugpt.models.nanogpt.get_model'
  module_kwargs:
    n_layer: 8
    n_embd: 512
    n_head: 8
    context_length: 512

    mlp_dropout: 0.2 # dropout for feedforward layer
    attn_dropout: 0.2 # dropout for attention layer
    resid_dropout: 0.2 # dropout for residual connection
    embed_dropout: 0.2 # dropout for embedding layer

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/tinyshakespeare/byte/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/tinyshakespeare/byte/val.bin'
    eval_batch_size: 64

tokenizer:
  _inherit: false # remove base config's settings
  module: 'nanugpt.tokenizers.byte_tokenizer.get_tokenizer_factory'
  module_kwargs:
    encoding_name: 'utf-8'

training:
  device_batch_size: 32
  max_steps: 80  # Karpathy's original value is 5000 but dataset overfits after 2300 steps for global batch size of 64
  global_batch_size: 2048

optimizer:
  module: 'nanugpt.optimizers.adamw_nanogpt.get_optim'
  module_kwargs:
    learning_rate: 2.5E-3 # LRRT found 2.5E-3, Karpathy used 1.0E-3
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.99

scheduler:
  module: 'nanugpt.schedulers.cosine.get_scheduler'
  module_kwargs:
    warmup_iters: 100
    max_iters: '_copy: /training/max_steps'
    end_factor: 1.0E-2

eval:
  eval_every: 250
  eval_iters: 200 # number of samples to evaluate for dataset
  save_checkpoint: false
  checkoint_after: 0 # starts saving checkpoint after these steps
  checkpoint_every_hr: 1 # multiple of eval_every, save checkpoint after these steps from last checkpoint when val loss is better
  checkpoint_keep_best: false # keep only the best checkpoint, otherwise keep all with  _{step}.pt
