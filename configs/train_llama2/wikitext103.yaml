__include__: ['base_config.yaml']

general:
  out_dir: '~/out_dir/llama2_124m_wikitext103'

logging:
  project_name: 'llama2_124m_wikitext103'
  enable_wandb: true
  enable_summaries: true
  run_name: 'llama2-124m_wikitext103_llama2_bloat16'  # select random name
  run_description: 'Llama2-124M WikiText-103 using llama2 tokenizer and bfloat16'

data:
  module_kwargs:
    tokenized_train_path: '$DATA_ROOT/tokenized/wikipedia/llama2/train.bin'
    tokenized_val_path: '$DATA_ROOT/tokenized/wikipedia/llama2/validation.bin'
    tokenized_test_path: '$DATA_ROOT/tokenized/wikipedia/llama2/test.bin'

model:
  module: 'nanugpt.models.nanogpt.get_model'
  module_kwargs:
    n_layer: 24
    n_embd: 1024
    n_head: 16
    context_length: 1024

training:
  train_batch_size: 64
  num_steps: 300000
  gradient_accumulation_steps: 8
  adj_grad_acc_gpu_count: true # adjust gradient accumulation steps for number of GPUs

optimizer:
  module: 'nanugpt.optimizers.adamw_nanogpt.get_optim'
  module_kwargs:
    learning_rate: 1.5E-4

scheduler:
  module: 'nanugpt.schedulers.nanogpt_cosine.get_scheduler'
  module_kwargs:
    warmup_iters: 3000
    min_lr: 1.0E-5
